---
layout: post
read_time: true
show_date: true
title:  这是用来测试文章的
description: "Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?"
img: posts/20210318/TicTacToeSml.jpg
tags: [machine learning, artificial intelligence, reinforcement learning, coding, python]
author: Armando Maynez
github: amaynez/TicTacToe/
toc: yes # leave empty or erase for no TOC
---


Article list with excerpt, read more link and info.

<!--more-->

**front matter:**

    ---
    layout: articles
    title: Articles - Item (Excerpt + Read More + Info)
    articles:
      data_source: site.sample_page
      show_cover: false
      show_excerpt: true
      show_readmore: true
      show_info: true
    ---

</div>






#		sklearn.svm.SVC()

```python
class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale',
 coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200,
  class_weight=None, verbose=False, max_iter=- 1,
   decision_function_shape='ovr', break_ties=False, random_state=None)[source]
```
**参数解析：**

<font color="#AA0000">英文解释：</font>

|C|float, default=1.0|Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.
|--|--|--
|kernel|{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’|Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).
|degree|int, default=3|Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.
|gamma|{‘scale’, ‘auto’} or float, default=’scale’|Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.       - - -if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,- - -if ‘auto’, uses 1 / n_features.Changed in version 0.22: The default value of gamma changed from ‘auto’ to ‘scale’.
|coef0|float, default=0.0|Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.
|shrinking|bool, default=True|Whether to use the shrinking heuristic. See the User Guide.
|probability|bool, default=False|Whether to enable probability estimates. This must be enabled prior to calling fit, will slow down that method as it internally uses 5-fold cross-validation, and predict_proba may be inconsistent with predict. Read more in the User Guide.
|tol|float, default=1e-3|Tolerance for stopping criterion.
|cache_size|float, default=200|Specify the size of the kernel cache (in MB).
|class_weight|dict or ‘balanced’, default=None|Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))
|verbose|bool, default=False|Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.
|max_iter|int, default=-1|Hard limit on iterations within solver, or -1 for no limit.
|decision_function_shape|{‘ovo’, ‘ovr’}, default=’ovr’|Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (‘ovo’) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one (‘ovo’) is always used as multi-class strategy. The parameter is ignored for binary classification.Changed in version 0.19: decision_function_shape is ‘ovr’ by default.New in version 0.17: decision_function_shape=’ovr’ is recommended.Changed in version 0.17: Deprecated decision_function_shape=’ovo’ and None.
|break_ties|bool, default=False|If true, decision_function_shape='ovr', and number of classes > 2, predict will break ties according to the confidence values of decision_function; otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict.
|random_state|int, RandomState instance or None, default=None|Controls the pseudo random number generation for shuffling the data for probability estimates. Ignored when probability is False. Pass an int for reproducible output across multiple function calls. See Glossary.

<font color="#AA0000">中文翻译：</font>

-	C：惩罚参数，惩罚松弛变量，默认值是1.0。C值大，对训练集分类更加正确。对训练集测试时准确率很高，但泛化能力弱。C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。

-	kernel ：核函数，默认是rbf，可以是‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’
			
		详见西瓜数p128
		0 – 线性：u’v
		1 – 多项式：(gamma*u’v + coef0)^degree
		2 – RBF函数：exp(-gamma|u-v|^2)
		3 –sigmoid：tanh(gammau’*v + coef0)

-	degree ：多项式poly函数的维度，默认是3，选择其他核函数时会被忽略。

-	gamma ： ‘rbf’,‘poly’ 和‘sigmoid’的核函数参数。默认是’auto’，则会选择1/n_features

-	coef0 ：核函数的常数项。对于‘poly’和 ‘sigmoid’有用。

-	probability ：是否采用概率估计。默认为False

-	shrinking ：是否采用shrinking heuristic方法，默认为true

-	tol ：停止训练的误差值大小，默认为1e-3

-	cache_size ：核函数cache缓存大小，默认为200

-	class_weight ：类别的权重，字典形式传递。设置第几类的参数C为weight*C(C-SVC中的C)

-	verbose ：允许冗余输出

-	max_iter ：最大迭代次数。-1为无限制。

-	decision_function_shape ：‘ovo’, ‘ovr’ or None, default=None3

-	random_state ：数据洗牌时的种子值，int值

主要调节的参数有：C、kernel、degree、gamma、coef0。
#		
#		SVM Margins Example


**Example:**
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210513201737113.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl81MzI3MjI0Nw==,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210513201743339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl81MzI3MjI0Nw==,size_16,color_FFFFFF,t_70)
上面的图说明了参数C对分离线的影响。C的值越大就会告诉我们的模型，我们对数据的分布没有那么大的信心，<font color="#AA0000">只考虑接近分离线的点</font>。

C的值越小则表示包含了更多/所有的观测值，允许使用该区域的所有数据计算边距。

**源码：**

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from sklearn import svm

# 创造40个离散点
np.random.seed(0)
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20

# 特征数量
fignum = 1

# 拟合模型
for name, penalty in (('unreg', 1), ('reg', 0.05)):

    clf = svm.SVC(kernel='linear', C=penalty)
    clf.fit(X, Y)

    # 得到分割超平面
    w = clf.coef_[0]
    a = -w[0] / w[1]
    xx = np.linspace(-5, 5)
    yy = a * xx - (clf.intercept_[0]) / w[1]

#绘制平行线到通过支持向量（方向上距超平面的边距）垂直于超平面）。这是垂直方向上的sqrt（1+a^2）
#二维。
 	#边界线
    margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))
    yy_down = yy - np.sqrt(1 + a ** 2) * margin
    yy_up = yy + np.sqrt(1 + a ** 2) * margin
    
	#绘制线、点和到平面最近的向量
        plt.figure(fignum, figsize=(4, 3))
    plt.clf()
    plt.plot(xx, yy, 'k-')
    plt.plot(xx, yy_down, 'k--')
    plt.plot(xx, yy_up, 'k--')
	
	#绘制离散点和边界点
    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
                facecolors='none', zorder=10, edgecolors='k',
                cmap=cm.get_cmap('RdBu'))
    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=cm.get_cmap('RdBu'),
                edgecolors='k')

    plt.axis('tight')
    x_min = -4.8
    x_max = 4.2
    y_min = -6
    y_max = 6

    YY, XX = np.meshgrid(yy, xx)
    xy = np.vstack([XX.ravel(), YY.ravel()]).T
    Z = clf.decision_function(xy).reshape(XX.shape)

	#把结果做成等高线图
    plt.contourf(XX, YY, Z, cmap=cm.get_cmap('RdBu'),
                 alpha=0.5, linestyles=['-'])

    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)

    plt.xticks(())
    plt.yticks(())
    fignum = fignum + 1

plt.show()
```
**具体的数学原理博主就不再敲了，随便找一本机器学习的书都有。**
观察上图，聪明的你会发现有一些点就在边界线上。这些点是拟合的<font color="#AA0000">**关键支持点**</font>，被称为<font color="#AA0000">**支持向量**</font>，<font color="#AA0000">**支持向量机算法**</font>也因此得名。



-	np.r_是按列连接两个矩阵，就是把两矩阵上下相加，要求列数相等。
-	np.c_是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等。
-	clf = classifier的缩写（分类器）具体可参照这篇文章：[sklearn包中的svm详解（coef_和intercept_）](https://blog.csdn.net/qq_43043256/article/details/104259061?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162090869716780274142160%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=162090869716780274142160&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-104259061.first_rank_v2_pc_rank_v29&utm_term=clf.coef_%5B0%5D&spm=1018.2226.3001.4187)
-	np.ravel()和np.flatten()	
		两者的功能是一致的，将多维数组降为一维，但是两者的区别是返回拷贝还是返回视图，np.flatten(0返回一份拷贝，对拷贝所做修改不会影响原始矩阵，而np.ravel()返回的是视图，修改时会影响原始矩阵
		

```python
import numpy as np
a = np.array([[1 , 2] , [3 , 4]])
b = a.flatten()
print('b:' , b)
c = a.ravel()
print('c:' , c)
d = a.ravel('F')
print('d:' , d)

# 二者的区别
b[0] = 10
print('a:' , a)
c[0] = 10
print('a:' , a)
————————————————————————————————————————————————————————————————
输出：
b: [1 2 3 4]
c: [1 2 3 4]
d: [1 3 2 4]
a: [[1 2]
[3 4]]
a: [[10 2]
[ 3 4]]
```


-	    plt.contourf(XX, YY, Z, cmap=cm.get_cmap('RdBu'),
                 alpha=0.5, linestyles=['-'])
这段代码作用是画出等高线图增强视觉
否则将像如下：
                                  ![在这里插入图片描述](https://img-blog.csdnimg.cn/2021051321075895.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl81MzI3MjI0Nw==,size_16,color_FFFFFF,t_70)


**核心代码：**

```python
  clf = svm.SVC(kernel='linear', C=penalty)
    clf.fit(X, Y)

    # 得到分割超平面
    w = clf.coef_[0]
    a = -w[0] / w[1]
    xx = np.linspace(-5, 5)
    yy = a * xx - (clf.intercept_[0]) / w[1]
#绘制平行线到通过支持向量（方向上距超平面的边距）垂直于超平面）。
#这是垂直方向上的sqrt（1+a^2）
#二维。
 	#边界线
    margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))
    yy_down = yy - np.sqrt(1 + a ** 2) * margin
    yy_up = yy + np.sqrt(1 + a ** 2) * margin
```
就是通过这段代码我们才得以将超平面求出。